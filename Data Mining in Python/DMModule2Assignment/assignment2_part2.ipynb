{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8783b067e7d492616519fa42cc3ad3f9",
     "grade": false,
     "grade_id": "cell-a7fb49948b186646",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: Mining Itemsets (Part II)\n",
    "\n",
    "## Finding Frequent Itemsets with Apriori\n",
    "\n",
    "In Part I of this assignment, the summary statistics gave us a brief view of what the data look like. Now it is time for the real business - let's use the *Apriori* algorithm to find the frequent itemsets. \n",
    "First, let's import the packages and dependencies that will be used in this part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "016fac9e539b990dcdfab076c0665b2c",
     "grade": false,
     "grade_id": "cell-44958ea23aaa6438",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "import time\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43697e9d960cb08cea94e11efc016c64",
     "grade": true,
     "grade_id": "cell-dbe07ab8099bc156",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Ignore warnings generated by mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c0a38569a4bd3911a6e580c189c72de",
     "grade": false,
     "grade_id": "cell-c1f2b51b69369878",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**<span style=\"color:red\">NOTE: These are all the imports we need to make for this assignment. You should not make other imports in your submitted notebook. You will receive 0 points for the exercises if your solution includes additional imports.</span>**\n",
    "\n",
    "Before you practice Apriori on the Instacart dataset, let's use another dataset as an example to get familiar with the algorithm. For this assignment, **we sampled 10,000 Tweets with two or more food/drink emojis** (yes, those colorful tasty ideograms). You will represent this dataset as a collection of itemsets and practice what we learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3d74132d53c1533fbf18a66e06c2fe2",
     "grade": false,
     "grade_id": "cell-0b9c1ae4a1d47df5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the tweets data from files.\n",
    "tweets_df = pd.read_csv(\"assets/food_drink_emoji_tweets.txt\", sep=\"\\t\",header=None)\n",
    "tweets_df.columns = ['text']\n",
    "\n",
    "# Only the emojis below are considered in this assignment.\n",
    "emoji_list = \"🍇🍈🍉🍊🍋🍌🍍🥭🍎🍏🍐🍑🍒🍓🥝🍅🥥🥑🍆🥔🥕🌽🌶🥒🥬🥦🍄🥜🌰🍞🥐🥖🥨🥯🥞🧀🍖🍗🥩🥓🍔🍟🍕🌭🥪🌮🌯🥙🥚🍳🥘🍲🥣🥗🍿🧂🥫🍱🍘🍙🍚🍛🍜🍝🍠🍢🍣🍤🍥🥮🍡🥟🥠🥡🦀🦞🦐🦑🍦🍧🍨🍩🍪🎂🍰🧁🥧🍫🍬🍭🍮🍯🍼🥛☕🍵🍶🍾🍷🍸🍹🍺🍻🥂🥃\"\n",
    "emoji_set = set(emoji_list)\n",
    "\n",
    "# Get a list of emojis used in each tweet.\n",
    "tweets_df['emojis'] = tweets_df.text.apply(lambda text:np.unique([chr for chr in text if chr in emoji_set]))\n",
    "\n",
    "# Create a matrix for Apriori algorithms.\n",
    "mlb = MultiLabelBinarizer()\n",
    "emoji_data = mlb.fit_transform(tweets_df.emojis).astype(bool)\n",
    "emoji_matrix = pd.DataFrame(data=emoji_data, index=tweets_df.index, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ace931a7603c72d9869df9d49a4a340",
     "grade": false,
     "grade_id": "cell-85ec828b9b80301a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can call the Apriori API now and specify the minimal support we want. You may learn more about this API from its [documentation](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "202081925e307ad47e1751be211677d8",
     "grade": false,
     "grade_id": "cell-a8ed42457c858b1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply the Apriori algorithms and find the most frequent items.\n",
    "freq_emoji_itemsets = apriori(emoji_matrix, min_support=0.005, use_colnames=True)\n",
    "freq_emoji_itemsets.sort_values(\"support\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdf2efec30c13babd3f1cb0aaa004e41",
     "grade": false,
     "grade_id": "cell-33456a784d1c91ea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "With the above command, we preserve all itemsets with a min support of 0.005 (half percent of the shopping baskets). We can now use the following command to extract the frequent itemsets with length 2 and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adbfdf475292261ab833a6c35194d015",
     "grade": false,
     "grade_id": "cell-39c767c1368f560e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply the Apriori algorithms and find the most frequent 2-itemsets.\n",
    "freq_emoji_itemsets[freq_emoji_itemsets[\"itemsets\"].apply(\n",
    "    lambda x: len(x) > 1)].sort_values(\"support\", ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6eaa2deb2833b19413450fff614e160a",
     "grade": false,
     "grade_id": "cell-0ea264f7b074a51a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Now, it is time to apply the Apriori algorithm to the Instacart dataset.**\n",
    "\n",
    "Since we have already shown you how to transform the Instacart dataset into itemsets, we concatenate the data preprocessing code into one block. Please run the following code block to load and preprocess the data. Notice that we limit the number of products in this problem to 100 by popularity so that the apriori algorithm will not run for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ab9f1f6e10537ce7e2d99bb691f6cfa",
     "grade": false,
     "grade_id": "cell-b21ffb5586247c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load data from files.\n",
    "orders = pd.read_csv(\"assets/orders.csv.zip\", nrows=10000)\n",
    "products = pd.read_csv(\"assets/products.csv.zip\")\n",
    "\n",
    "# Group orders by order id and merge them into a list.\n",
    "order_baskets = orders.groupby(\"order_id\")[\"product_id\"].apply(list)\n",
    "\n",
    "# Convert the above pandas Series to a pandas DataFrame.\n",
    "order_baskets = order_baskets.to_frame(name=\"products_id\")\n",
    "\n",
    "# Create the name map for later reference.\n",
    "product_name_map = dict(zip(products[\"product_id\"], products[\"product_name\"]))\n",
    "\n",
    "# Create the matrix like Part 1.\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = mlb.fit_transform(order_baskets[\"products_id\"]).astype(bool)\n",
    "prod_matrix = pd.DataFrame(\n",
    "    data=data, index=order_baskets.index, columns=mlb.classes_\n",
    ")\n",
    "prod_popularity = prod_matrix.sum(axis=0)\n",
    "prod_matrix = prod_matrix.astype(bool)\n",
    "top_prods = prod_popularity.sort_values(ascending=False).head(100).index\n",
    "prod_matrix = prod_matrix[top_prods]\n",
    "prod_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9382d406a650bcff2725cc804cd4ef46",
     "grade": false,
     "grade_id": "cell-cb0746931093042d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prod_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5df59f330cda2c2d79874b133cd198d",
     "grade": false,
     "grade_id": "cell-6df5585ce9a99cfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.  (20 pts)\n",
    "Complete the following `prod_frequent_itemsets` function to find all the **frequent *k*-itemsets** with a minimal support of **`min_support`** in the Instacart dataset. \n",
    "\n",
    "Your function should return a Pandas DataFrame object like the `freq_emoji_itemsets` object above, being the default format returned by the `apriori` function. \n",
    "\n",
    "Make sure that you are only returning the frequent itemsets that have the specified number of products (`k`).\n",
    "\n",
    "Hint: \n",
    "\n",
    "Code in the previous blocks shows you how to obtain itemsets with a length of greater than 1. How might you change the section of the code `freq_emoji_itemsets[freq_emoji_itemsets[\"itemsets\"].apply(lambda x: len(x) > 1)]` to instead obtain itemsets equal to (==) k instead of greater than (>) 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e919bf4d66c2e49c9deb502733b3d789",
     "grade": false,
     "grade_id": "cell-d60cab66db769bd1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prod_frequent_itemsets(prod_matrix, min_support=0.005, k=3):\n",
    "    freq_sets = \"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return freq_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74d04937b018bd9529eb5aa0e89a8409",
     "grade": false,
     "grade_id": "cell-32f0579c8e0af34c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If you implemented this function correctly, we can obtain all frequent 2-itemsets with a min support of 0.004 by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c9039b804f9527cbe5ae0aa16bbe384",
     "grade": false,
     "grade_id": "cell-aedc2547d9eba516",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prod_frequent_3itemsets = prod_frequent_itemsets(prod_matrix, min_support=0.004, k=3)\n",
    "prod_frequent_3itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a7469f592ebfb8bd890c4848172183c",
     "grade": true,
     "grade_id": "cell-c6ca10bd0deb9c39",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prod_frequent_3itemsets = prod_frequent_itemsets(prod_matrix,\n",
    "                                                 min_support=0.004,\n",
    "                                                 k=3)\n",
    "for row in prod_frequent_3itemsets.itertuples():\n",
    "    assert row.support >= 0.004, f\"[Exercise 2] The support of the itemset {row.itemsets} is below the threshold.\"\n",
    "    assert len(row.itemsets) == 3, f\"[Exercise 2] The itemset {row.itemsets} is not a 3-itemset.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67888b4d5da4a510b898b0f7e591b1df",
     "grade": false,
     "grade_id": "cell-e851d702b96a2720",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's replace the product IDs with their names. Does the result make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41380fdd610f238e7e86f0850dfb8a0d",
     "grade": false,
     "grade_id": "cell-7f4304d769ea73a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prod_frequent_3itemsets.itemsets = prod_frequent_3itemsets.itemsets.apply(lambda row: [product_name_map[i] for i in row])\n",
    "prod_frequent_3itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed0cf9c6055eef431b3efe7f22603aed",
     "grade": false,
     "grade_id": "cell-a9b142fc147a4f1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluating Frequent Itemsets (Part III.)\n",
    "\n",
    "Even though we have found all the frequent itemsets, not all of them are interesting. In Part IV of this assignment, we will practice how to evaluate the frequent itemsets.\n",
    "\n",
    "People have developed various measurements of the interestingness of patterns. Most of them split the itemset into an antecedent item(set) and a consequent item(set), and then measure the correlation between the antecedent and the consequent. Let's try some of such measurements implemented by the `mlxtend.frequent_patterns.association_rules` API, which we have imported. For more information about the API, visit the [documentation](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/) of the `mlxtend` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19681f9d2b348ac5f9c74000e06c7ba8",
     "grade": false,
     "grade_id": "cell-4c0992085cf36ebb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prod_frequent_itemsets_basic = apriori(prod_matrix, min_support=0.005, use_colnames=True)\n",
    "\n",
    "\n",
    "\n",
    "interestingness_measurements = association_rules(prod_frequent_itemsets_basic, metric=\"lift\", min_threshold=0)\n",
    "interestingness_measurements.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "993532c9b0694376db41255d28eec476",
     "grade": false,
     "grade_id": "cell-52e3cd60602876d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the returned data frame, each row examines one (antecedent -> consequent) pair. *Antecedent support* and *consequent support* measure *P*(antecedent) and *P*(consequent), while *support* measures *P*(antecedent, consequent). In fact, these three values help us characterize the $2\\times2$ contingency table, as illustrated in the following table:\n",
    " \n",
    " |           |              |       |              | \n",
    " -----------:|:------------:|-------|---------------\n",
    " |           |    X = 1     | X = 0 |   sum(row)   |\n",
    " |     Y = 1 |   `support`    |       | `cons_support` |\n",
    " |     Y = 0 |              |       |              |\n",
    " | sum(col.) | `ante_support` |       |              |\n",
    "\n",
    "Most interestingness measurements, including the four shown in the data frame (*confidence*, *lift*, *leverage*, and *conviction*), can be derived from the three support values. For example, $$\\text{confidence}=\\frac{\\text{support}}{\\text{antecedent\\_support}},$$ and $$\\text{lift} =\\frac{\\text{confidence}}{\\text{consequent\\_support}}=\\frac{\\text{support}}{\\text{antecedent\\_support} * \\text{consequent\\_support}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "131c0c867e8e073cb3e3248f34ad8379",
     "grade": false,
     "grade_id": "cell-2b9e5f3c3cb469eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3. (15 pts)\n",
    "In this exercise, we are going to implement another interestingness\n",
    "measurement, the (full) mutual information which we will use when adding a 'mutual information'\n",
    "column to the data frame. The measurement is defined as\n",
    "\n",
    "$$I(X;Y)=\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}} P(X=x,\n",
    "Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}.$$\n",
    "\n",
    "(*The [mutual information](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "quantifies the \"amount of information obtained from one random variable by\n",
    "observing the other random variable.*)\n",
    "\n",
    "Note that the logorithm requirest that the joint probability $P(X=x, Y=y) > 0$,\n",
    "which does not hold for some $(x, y)$. However, since we know that when $P(X=x,\n",
    "Y=y) = 0$, it would not contribute to the sum, you may assume $P(X=x,\n",
    "Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(P=y)} = 0$ in that case. \n",
    "\n",
    "$x$, $y$ are possible values of $X$ and $Y$; in the case of appearance or\n",
    "absence of an item, 1 or 0. Therefore, we need to consider all possible\n",
    "combinations of $x$ and $y$, that is, $(X=1, Y=1)$, $(X=1, Y=0)$, $(X=0, Y=1)$,\n",
    "$(X=0, Y=0)$.\n",
    "\n",
    "Please complete the following function that uses the three support values to\n",
    "compute the mutual information and return it as a float. All the three parameters are in [0, 1], and you\n",
    "can assume the validity of the input. **Use 2 as the log base.** We have\n",
    "created some auxilary variables for you, each represent a joint or marginal\n",
    "(let $X$ denote the antecedent item and $Y$ denote the consequent item)\n",
    "probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6416e0461df6a2d099e8f8449527cfe4",
     "grade": false,
     "grade_id": "cell-abe960a668a149d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mi(antecedent_support, consequent_support, support):\n",
    "       \n",
    "    px1 = antecedent_support\n",
    "    px0 = 1 - antecedent_support\n",
    "    py1 = consequent_support\n",
    "    py0 = 1 - consequent_support\n",
    "    \n",
    "    px1y1 = support\n",
    "    px1y0 = px1 - px1y1\n",
    "    px0y1 = py1 - px1y1\n",
    "    px0y0 = 1 - px1 - py1 + px1y1\n",
    "    \n",
    "    mutual_information = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aba482e8cc563a380a240717f02b609",
     "grade": true,
     "grade_id": "cell-4707d6c80200e6cf",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This code block tests whether the `mi` function work as expected.\n",
    "# We hide some tests, so passing all the displayed assertions does not guarantee the bonus points.\n",
    "\n",
    "assert np.abs(mi(0.6, 0.75, 0.4) - 0.04287484674660057) < 1e-8\n",
    "assert np.abs(mi(0.5, 0.5, 0.25) - 0) < 1e-8\n",
    "\n",
    "# If you fail the following assertion, double check if your function \n",
    "# handles the scenarios in which a joint probability is zero.\n",
    "assert np.abs(mi(0.5, 0.5, 0.5) - 1) < 1e-8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cef1b0b302005d275f966e70a0524b0c",
     "grade": false,
     "grade_id": "cell-4b9a6d192de296d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With the `mi` function, we can now compute the mutual information for each (antecedent -> consequent) pair and attach it to the data frame. Does the result make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "863ce77c1ef389acc7feda7b194497f3",
     "grade": false,
     "grade_id": "cell-584bae38112c64ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mutual information is a classical measure of interestingness. We encourage you to think about the following questions (not graded):\n",
    "1. What is the maximum of mutual information in this setting? How to reach it?\n",
    "2. What is the minimum of mutual information in this setting? How to reach it?\n",
    "\n",
    "What does the max/min value imply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a881ce0b249761f910b62e4bccf5ee6",
     "grade": false,
     "grade_id": "cell-bb5692d22c5a4a31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "interestingness_measurements['mi'] = \\\n",
    "    interestingness_measurements.apply(lambda pair: mi(pair['antecedent support'], \n",
    "                                              pair['consequent support'], \n",
    "                                              pair['support']),\n",
    "                                       axis=1)\n",
    "interestingness_measurements.sort_values('mi', ascending=False).head(n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
